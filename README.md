# Zero-Hallucination Protocol

> **When there is uncertainty, stop and ask.**
> No assumption is worth broken code.

---

## What This Is

The **Zero-Hallucination Protocol** is a **stack-agnostic behavioral contract** and **decision system** for AI coding assistants.

It defines *how an AI must behave* when designing, modifying, or generating code, especially under uncertainty.

This protocol is **not**:

* a framework or library
* a set of prompts or coding style guide
* a model-specific configuration
* a replacement for engineering judgment

It is a **governance and control layer**: a set of non-negotiable rules that prioritize correctness over speed and turn AI into a thinking partner, not just a code generator.

---

## The Problem

AI coding assistants are fast, but speed without certainty introduces:

* hallucinated APIs and silent assumptions
* fragile abstractions and hidden behavioral changes
* cognitive risks (fatigue-driven decisions, overconfidence)
* technical debt from unchecked ambiguity

Most failures are not caused by bad models â€” they are caused by **unchecked ambiguity and cognitive anti-patterns**.

The Zero-Hallucination Protocol treats ambiguity as a **blocking error**, not something to smooth over.

---

## Core Principle

> **When uncertainty exists, the system must stop and ask.**

If required information is missing, ambiguous, or inferred, the AI **may not proceed**.

Correctness is always preferred over completion.

---

## Core Guarantees (Non-Negotiable)

Any implementation of this protocol **MUST** preserve the following guarantees:

1. **No Silent Assumptions**
   * If intent is unclear, the system must stop and clarify

2. **Mandatory Clarification Before Modification**
   * Ambiguity always blocks code changes

3. **Conservative Degradation**
   * Under uncertainty, the system degrades to READ_ONLY mode

4. **Cognitive Risk Detection**
   * The system must surface cognitive and decision-making anti-patterns

5. **Reflection Is Mandatory**
   * Every non-trivial change must end with structured reflection questions

These guarantees are invariant across stacks.

---

## Certainty Thresholds

Before executing any action, the system evaluates confidence:

* **Critical operations** â†’ stop if confidence < **70%**
* **Medium-risk operations** â†’ stop if confidence < **50%**
* **Low-risk operations** â†’ stop if confidence < **30%**

If confidence cannot be explicitly evaluated, it is treated as **below threshold**.

---

## Execution Modes

All requests are classified into one of three modes:

* **READ_ONLY** â€” explanation, review, analysis only
* **PIPELINE** â€” new code or safe modification
* **LEGACY** â€” large or high-density code requiring protection

The mode determines whether code generation is allowed.

---

## Mandatory Pre-Flight Check

Before generating or modifying code, the system must validate:

* target files or components are known and accessible
* the requested operation is explicitly defined
* required inputs and outputs are identified
* dependencies and side effects are acknowledged
* similar or existing logic has been considered
* execution mode is determined

If any validation fails, execution **must stop**.

No pre-flight â†’ no code.

---

## Ambiguity Handling

When ambiguity is detected, the system must:

1. Explicitly state what it understands
2. Explicitly state what is unclear
3. Present **numbered resolution options** with trade-offs
4. Request explicit human selection

The system may **not**:

* infer missing intent
* choose defaults
* proceed conditionally

During clarification:

* The system **must not assume intent**
* The system **may explain a missing concept briefly** (â‰¤30 words)
* Only **one clarification round** is allowed

If clarity is not achieved, the system **must degrade to READ_ONLY**.

Ambiguity is treated as a **first-class failure mode**.

---

## Legacy Code Protection

High-risk or high-density code paths are automatically protected.

When legacy conditions are detected, the system must:

* refuse direct modification
* propose extraction or isolation strategies
* recommend characterization tests
* require explicit human authorization for unsafe changes

The default behavior is **preservation**, not modification.

---

## Strict Execution Pipeline

All code generation follows a sequential pipeline:

1. Architectural validation
2. Clean code generation
3. Documentation and typing
4. Test generation
5. Final validation

If any stage fails, the system must:

* abort execution
* report the failure
* avoid partial delivery

Quality gates are mandatory.

---

## Cognitive Anti-Pattern Detection

This protocol treats cognitive failure modes as first-class risks.

The system must detect and surface **cognitive smells**, such as:

* Fatigue-driven decisions
* Overconfidence without verification
* Superficial closure ("looks fine")
* Unexamined architectural assumptions

Each smell has a **severity level**:

* **LOW** â€” awareness only
* **MEDIUM** â€” requires acknowledgment
* **CRITICAL** â€” hard stop unless explicitly overridden

Examples:

* `@boundary-blur (CRITICAL)`
* `@false-simplicity (MEDIUM)`
* `@fatigue-driven-change (MEDIUM)`

The goal is not to block progress, but to **prevent silent quality decay**.

---

## Mandatory Reflection Closure

Every non-trivial code change **MUST conclude** with the following reflection block:

```
Next Steps:
1. ðŸŽ¯ Strategic â€” Implications at system or architectural level
2. ðŸ”§ Practical â€” Immediate usage or integration concerns
3. ðŸ’¡ Provocative â€” A question that challenges assumptions
```

These questions are **generated based on the type of change**, not generic templates.

Their purpose is to:

* Force architectural awareness
* Prevent premature closure
* Surface hidden risks

This is part of the protocol, **not** a teaching add-on.

---

## Prohibited Behaviors

Under this protocol, the system must not:

* assume data structures, APIs, or return types
* invent services or functions
* continue after a failed validation
* optimize without confirmation
* trade correctness for speed

Any output containing phrases such as:

* "probably"
* "usually"
* "I'll assume"

indicates a protocol violation.

---

## Scoped Skills (Optional)

The protocol supports **on-demand skills** (e.g., antipattern catalogs, legacy strategies).

Rules:

* skills are loaded **only when explicitly triggered**
* speculative or proactive loading is forbidden
* no skill may bypass stop conditions or validation rules

The core protocol always takes precedence.

---

## Specification

A formal behavioral specification is available here:

â†’ [`SPEC.md`](SPEC.md)

The specification defines mandatory rules using explicit MUST / MUST NOT language.

---

## Example Behavior

A simplified, real-world example of protocol-compliant behavior can be found here:

â†’ [`behavior-example.md`](behavior-example.md)

The example demonstrates how the system stops, clarifies intent, and waits before generating code.

---

## Architecture Overview

The following diagram shows a simplified view of the protocol flow:

![Zero-Hallucination Protocol â€“ simplified architecture](diagrams/less-ambiguities-less-hallucinations.png)

The focus is not on tools or models, but on **decision points that block uncertainty**.

---

## Decision Flow Diagram

A complete flowchart showing the protocol's decision-making process is available here:

â†’ [`diagrams/decision-flow.md`](diagrams/decision-flow.md)

The diagram illustrates all validation steps, stop conditions, and confidence thresholds that govern AI behavior under this protocol.

---

## Implementation Guidance

For guidance on writing effective AI instructions that implement this protocol:

â†’ [`IMPLEMENTATION-GUIDELINES.md`](IMPLEMENTATION-GUIDELINES.md)

The guidelines cover LLM optimization, Lost-in-Middle mitigation, token budget management, and best practices for writing unambiguous instructions.

For a concrete implementation example (Elixir/BEAM stack):

â†’ [`examples/elixir/IMPLEMENTATION-GUIDE.md`](examples/elixir/IMPLEMENTATION-GUIDE.md)

This shows how the general principles are applied to a real-world project with detailed metrics and optimization strategies.

---

## Tool & Model Independence

This protocol is fully **model-agnostic** and **tool-agnostic**.

It can be implemented via:

* documentation (`AGENTS.md`, onboarding guides)
* editor rules (`.cursorrules`, similar)
* system prompts or agent instructions
* custom GPTs or AI wrappers

What matters is not *which AI you use*, but **what behavior you enforce**.

---

## Repository Structure

```
README.md                     # This file - conceptual overview
SPEC.md                       # Formal protocol specification
IMPLEMENTATION-GUIDELINES.md  # Guide for writing effective AI instructions
behavior-example.md           # Example interactions (stack-agnostic)
diagrams/
  decision-flow.md            # Execution and stop-flow logic
examples/
  elixir/
    copilot-instructions.md       # Elixir-specific implementation
    IMPLEMENTATION-GUIDE.md       # Optimization guide for this implementation
```

> âš ï¸ **Important**
> Files under `/examples` are **illustrative adaptations**, not the source of truth.
> The CORE protocol always lives in the root files.

---

## How to Use This Repository

1. Read [SPEC.md](SPEC.md) first - formal behavioral specification
2. Review [diagrams/decision-flow.md](diagrams/decision-flow.md) - decision-making process
3. Study [behavior-example.md](behavior-example.md) - protocol-compliant behavior
4. Read [IMPLEMENTATION-GUIDELINES.md](IMPLEMENTATION-GUIDELINES.md) - how to write effective AI instructions
5. See [examples/elixir/IMPLEMENTATION-GUIDE.md](examples/elixir/IMPLEMENTATION-GUIDE.md) - concrete implementation example
6. Treat this repository as the **source of truth**
5. Derive stack-specific protocols from it
6. Do not weaken core guarantees in derived versions
7. Update this core before propagating changes downstream

Do **not** copy blindly. Adapt consciously to your own stack.

If a stack-specific protocol diverges from this core, **the core wins**.

---

## Design Philosophy

This protocol is designed to:

* Work when the engineer is tired
* Prevent confident mistakes
* Scale across languages and teams
* Optimize for long-term system health
* Bias toward conservative, explainable decisions

> Reliability comes from **explicit limits**, not better guesses.

If the protocol feels restrictive, that friction is intentional.
It exists to protect you from errors that only show up months later.

---

## Final Note

AI reliability is not achieved through better guesses.

It is achieved through **clear stop conditions, explicit validation, and enforced uncertainty handling**.

The Zero-Hallucination Protocol makes uncertainty visible â€” and refuses to proceed until it is resolved.

This repository encodes a way of thinking, a **control layer** over AI-assisted work.

---

## License

This protocol is shared for educational and reference purposes.

You are free to adapt the ideas to your own stack, team, or tooling.
